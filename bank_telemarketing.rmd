---
title: "Bank Marketing Campaign"
author: "Miguel Ángel Jiménez Sánchez"
date: "May the 30th, 2019"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org"); library(tidyverse)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org"); library(caret)
if(!require(Rborist)) install.packages("Rborist", repos = "http://cran.us.r-project.org"); library(Rborist)
if(!require(e1071))  install.packages("e1071", repos = "http://cran.us.r-project.org");library(e1071)
if(!require(klaR))  install.packages("klaR", repos = "http://cran.us.r-project.org");library(klaR)
if(!require(pROC))  install.packages("pROC", repos = "http://cran.us.r-project.org");library(pROC)
if(!require(mice))  install.packages("mice", repos = "http://cran.us.r-project.org");library(mice)
if(!require(VIM))  install.packages("VIM", repos = "http://cran.us.r-project.org"); library(VIM)
if(!require(glmnet))  install.packages("glmnet", repos = "http://cran.us.r-project.org"); library(glmnet)
if(!require(fastICA))  install.packages("fastICA", repos = "http://cran.us.r-project.org"); library(fastICA)
if(!require(kernlab))  install.packages("kernlab", repos = "http://cran.us.r-project.org"); library(kernlab)
if(!require(randomForest))  install.packages("randomForest", repos = "http://cran.us.r-project.org"); library(randomForest)


dl <- tempfile()
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip", dl)
unzip(dl, "bank-additional/bank-additional-full.csv")
rm(dl)
calls <- read.csv("./bank-additional/bank-additional-full.csv",sep=";")

```

# 1. Overview

## 1.1 Objective

Our goal on this project is to study a dataset generated in a marketing campaign. This campaign was performed by a Portuguese bank between 2008 and 2010 to offer a term deposit. 

We will try to obtain insights to optimize the resources of the marketing department. We will construct as well a predictive model to predict which contacts or calls (I'll use both terms) may result on the subscription to the product that we are offering.


## 1.2 The dataset

The dataset used on this project is based on "Bank Marketing" UCI dataset and is publicly available for research. It can be downloaded from <http://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip>.

The zip file contains two csv files. The first (bank-additional-full.csv) contains the full data set. The second file (bank-additional.csv) contains a sample of 10% of the former. The zip file contains as well a txt file with information about the dataset and a description of the fields. 

We will use only the file "bank-additional-full.csv". Each row represents a contact with a customer. The dataset contains 41188 observations and 21 variables, including our target variable 'y'. This variable is categorical with two possible values "yes" or "not", depending on whether the customer subscribes the term deposit or not.

The rest of the variables are described on the txt file. These variables can be divided in four groups:

1. Bank client data
  * age (numeric)
  * job: Type of job (categorical)
  * marital: marital status (categorical)
  * education (categorical)
  * default: has credit in default? (categorical)
  * housing: has housing loan? (categorical)
  * loan: has personal loan? (categorical)
2. Last contact of the current campaign
  * contact: contact communication type (categorical: "cellular","telephone")
  * month (categorical)
  * day_of_week (categorical)
  * duration: duration of call in seconds (numerical)
3. Social and economic context attributes
  * emp.var.rate: employment variation rate - quarterly indicator (numeric)
  * cons.price.idx: consumer price index - monthly indicator (numeric)     
  * cons.conf.idx: consumer confidence index - monthly indicator (numeric)     
  * euribor3m: euribor 3 month rate - daily indicator (numeric)
  * nr.employed: number of employees - quarterly indicator (numeric)
4. Other attributes regarding previous contacts with the client
  * campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
  * pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
  * previous: number of contacts performed before this campaign and for this client (numeric)
  * poutcome: outcome of the previous marketing campaign (categorical
5. The output variable y: has the client subscribed a term deposit? (binary: "yes","no")
 
 As we see on this dataset we have both categorical and numeric variables.

## 1.3 Main steps

The first step will be an exploratory data analysis. From this data analysis we'll get our first insights on this data set and we'll determine if some preprocessing is needed.

After that, we'll train 6 different algorithms with this algorithm: glm, glmnet, Naive Bayes, Knn, Rborist and SVM radial. Finally, we'll ensemble the probabilities obtained from these 6 algorithms into our final model.


# 2. Analysis/ Methods

The intent in this chapter is not doing an exhaustive review of all the variables on the dataset. It will be centered on the variables with the higher (or lower) predictive power and the variables that may need some preprocessing.

## 2.1 Duration variable

One of the first decisions on this dataset is removing the column 'Duration', which describes the duration of the call. Actually some tests that I've done result that this is a variable that helps greatly in predicting our target variable y. 

But it has a big inconvenient: we know the duration of the call at the same time than our target variable (indeed, if our call operators are polite enough to say "thanks" and "goodbye", the duration is known later than y). So this is not a variable that we can use for our objective to predict the subscriptions, as it is warned on the txt file included beside the csv file. 

## 2.2 Summary of the data

With the next command we'll see a summary of our data:

```{r}
summary(calls)
```

From here we can see several things:

* We have no NA values in our data. But some categorical variables have values as 'unknown' or 'nonexistent'. We'll analyse these variables later.
* The results of our target variable y aren't balanced. We only have roughly a 11% of 'yes'.
* Variable "default": in our data set of more than 40.000 calls we have only three calls to people with a previous default. The rest are unknown or have no previous default. It seems that this variable won't have a high predictive power.

## 2.3 Correlation on numeric columns

Now let's focus on the correlation of the numeric columns:

```{r}
# Get correlation matrix
(cormat <- Filter(is.numeric,calls) %>%
    cor(method="pearson"))

library(reshape2)
melted_cormat <- melt(cormat)

melted_cormat %>%
  ggplot(aes(x=Var1, y=Var2, fill=value)) +
  geom_tile(color="white") +
  scale_fill_gradient2(low="blue", high = "red", mid="white",
                       midpoint=0, limit= c(-1,1), space="Lab",
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle=45, vjust = 1, hjust=1)) +
  coord_fixed()

```

On this figure we can see strong correlations on several economic variables. Namely we have three variables strongly correlated: The euribor3m, which is an interest rate, and the two variables regarding employment (nr.employed and emp.var.rate). The correlation between the euribor3m and emp.var.rate is 0.9722, so one of these variables can be removed.  

## 2.4 Variable importance

Now we will study the variable importance on predicting our target value. To do this we'll use the availability of this functionality on the randomforest package. Let's generate 100 trees and see which are the most important variables:

```{r}
library(randomForest)
calls_y = calls$y
calls_x <- subset(calls, select=-c(duration,y))

rf <- randomForest(calls_x, calls_y, ntree = 100)
imp <- as.data.frame(importance(rf))
variables <- names(calls_x)
imp <- cbind(imp,variables) %>%
  arrange(desc(MeanDecreaseGini))
# set variables as factor to maintain orden in the plot
imp$variables <- factor(imp$variables, levels=imp$variables)
ggplot(imp, aes(x=variables,y=MeanDecreaseGini)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle=45, vjust = 1, hjust=1))

```

### 2.4.1 euribor 3m variable

One of the most important variables is an interest rate indicator as the 3 month euribor. It seems logic, as the product we are offering is a term deposit. Let's see the distribution of results regarding this indicator:

```{r}
calls %>%
  ggplot(aes(x=euribor3m, fill=y)) +
    geom_histogram(bins=6)
```

Seems that our campaign was more successful with a low interest rate. And we seldom have results with euribor around 3%. Why? Our data set register calls from may 2008 to November 2010. If we look at historical data for years [2008](https://www.euribor-rates.eu/euribor-2008.asp?i1=6&i2=6), [2009](https://www.euribor-rates.eu/euribor-2009.asp?i1=6&i2=6) and [2010](https://www.euribor-rates.eu/euribor-2010.asp?i1=6&i2=6) at <https://www.euribor-rates.eu>, we can see that interest rates collapsed quite abruptly at the end of 2008. Seems than some characteristic of our term deposit was more attractive with a low interest rate. Maybe our interest rate was competitive or maybe a relatively safe investment as a term deposit is preferred on crisis times.

### 2.4.2 age variable

The next figure shows the distribution of the calls by age, in bins of five years.

```{r}
calls %>%
  ggplot(aes(x=age, fill=y)) +
  geom_histogram(binwidth=5)
```

We can see that the age ranges have different success rates. Seems that our campaign was less successful with people between 35 and 45 years. The next figure shows the success rate by age:

```{r}
age_success <- calls %>%
  mutate(age_round = floor(age/10)*10, bin=paste(age_round, age_round+10,sep="-")) %>%
  group_by(bin,y) %>%
  summarize(n=n()) %>%
  ungroup() %>%
  group_by(bin) %>%
  spread(y,n, fill=0) %>%
  mutate(success_percent = (yes /(no+yes))*100)

age_success %>%
  ggplot(aes(x=bin, y=success_percent)) +
  geom_bar(stat="identity")
```

### 2.4.3 job variable

Now we'll analyse the job variable. In the next figure we classify the calls by jobs, and show the successes and failures by color.

```{r}
calls %>%
  ggplot(aes(x=job, fill=y)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle=45, vjust = 1, hjust=1))
```

Now the success rates by job are calculated. We'll show the best and the worse rates.

```{r}
job_success <- calls %>%
  group_by(job,y) %>%
  summarize(n=n()) %>%
  ungroup() %>%
  group_by(job) %>%
  spread(y,n) %>%
  mutate(success_percent = (yes /(no+yes))*100) %>%
  arrange(desc(success_percent))
```

The best rates:

```{r}
head(job_success,5)
```

Seems that the campaign was quite successful on students and retired people. Let's see the worse rates:

```{r}
tail(job_success,5)
```

The worse results were with blue-collar employees.

### 2.4.4 Education

The next figure shows the success ratios by education level:
```{r}
calls %>%
  ggplot(aes(x=education, fill=y)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle=45, vjust = 1, hjust=1))
```

We can see here that we have a category (illiterate) with very few observations. A way to address this may be to fusion this category with the lower education level, basic.4y, that now will be this category or lower. We'll do it this way:

```{r}
# Fusion of illiterate and basic.4y
levels_ed <- levels(calls$education)
levels(calls$education) <- ifelse(levels_ed=="illiterate","basic.4y",levels_ed)
rm(levels_ed)
```

And now the figure is:

```{r}
calls %>%
  ggplot(aes(x=education, fill=y)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle=45, vjust = 1, hjust=1))
```

The next table shows the success ratios by education level

```{r}
education_success <- calls %>%
  group_by(education,y) %>%
  summarize(n=n()) %>%
  ungroup() %>%
  group_by(education) %>%
  spread(y,n) %>%
  mutate(success_percent = (yes /(no+yes))*100) %>%
  arrange(desc(success_percent))

education_success
```

Apart from the 'unknown' category, the customers with the highest education levels seems to be a little more likely to subscribe the term deposit.

Regarding the 'unknown' category, seems that has a reasonable rate of success. Indeed it is the category with the higher rate of success. It doesn't seem reasonable to consider not knowing the education level of our customers as a success factor in our campaign, so we'll need to apply here some imputation techniques. We'll review this in the preprocessing chapter.


## 2.5 Variables with near Zero variance or near Zero entropy 

The caret package has a function that detects features with near zero variance, and hence candidates to be removed. Let's see its recommendation.

```{r}
nzv <- nearZeroVar(calls)
names(calls)[nzv]
```

The recommendation is to remove the 'pdays' variable which is the number of days passed since the last contact with the same client. But If we we look at the bar plot on the previous point, this variable has a worthy ninth place of nineteen. Let's have a closer look to this variable.

### 2.5.1 variable 'pdays'

The next table shows how many rows we have for each value in pdays:

```{r}
table(calls$pdays)
```

If we calculate the frequency of the value '999' (no previous contact):

```{r}
calls %>%
  group_by(pdays) %>%
  summarize(n = n()) %>%
  mutate(freq = (n / sum(n)) * 100) %>%
  filter(pdays==999) %>% .$freq
```

We obtain that 96 % of the calls have no previous contact. But remember that our target variable is asymmetric as well. Let's the relationship between this variable with other values than 999 and our target variable.

```{r}
calls %>%
  filter(pdays != 999) %>%
  group_by(y) %>%
  summarise(n=n())
```

As we see, roughly two thirds of the calls with a recent contact have a positive result. Indeed, comparing with the total number of positive results in our data set we see that approximately a 20% of the positive results have previous contacts in the lasts days.

Given these results, we'll keep the pdays variable, and recommend our marketing department to place resources on following up some customers.

### 2.5.2 variable 'default'

Now we'll analyse the 'default' variable. As we saw with the summary only three calls corresponded to customers with a previous default. We saw as well that in the first bar plot on the variable importance point this variable had the last position. 

This is a categorical variable. Just to give a number to discard this variable, we'll use the entropy formula defined by Shannon:

$$H(x) = -\sum_{x_i}p(x_i)log_2p(x_i)$$

In r code: 

```{r}
entropy = function(vector) {
  px = table(vector) / length(vector)
  lpx = log(px, base=2)
  -sum(px * lpx)
}
```

Our entropy is:

```{r}
entropy(calls$default)
```

But after removing the unknown values we have near zero entropy:

```{r}
entropy(droplevels(calls$default[calls$default != "unknown"]))
```

So this is clearly a variable candidate to be not considered in our model.

## 2.6 Missing values

As we have seen in previous steps, some categorical variables have values labeled as 'unknown' or 'nonexistent'.

The only variable that has the value 'nonexistent' is the variable 'poutcome' (previous outcome). By my understanding, these aren't actually missing values so I won't do any imputation on them.

The next five categorical variables have value labeled as 'unknown': education, housing, loan, job and marital. The next figures shows some patterns regarding unknown data on these data:


```{r}
## Missing values
# replace unknown by NA

calls$job <- as.character(calls$job)
calls$job <- ifelse(calls$job == "unknown", NA, calls$job) 
calls$job <- factor(calls$job)

calls$marital <- as.character(calls$marital)
calls$marital <- ifelse(calls$marital == "unknown", NA, calls$marital) 
calls$marital <- factor(calls$marital)

calls$education <- as.character(calls$education)
calls$education <- ifelse(calls$education == "unknown", NA, calls$education) 
calls$education <- factor(calls$education)

calls$housing <- as.character(calls$housing)
calls$housing <- ifelse(calls$housing == "unknown", NA, calls$housing) 
calls$housing <- factor(calls$housing)

calls$loan <- as.character(calls$loan)
calls$loan <- ifelse(calls$loan == "unknown", NA, calls$loan) 
calls$loan <- factor(calls$loan)


aggr_plot <- aggr(calls[,c("job","marital","education", "housing", "loan")], col=c('navyblue','yellow'),
                  numbers=TRUE, sortVars=TRUE, prop=FALSE,
                  labels=c("job","marital","education", "housing", "loan"), cex.axis=.7,
                  gap=3, ylab=c("Missing data", "Pattern"))
```

These figures give us some insights on missing data:

1. 38245 observations out of 41188 have all 5 variables informed
2. There aren't observations with all 5 variables 'unknown'
3. The variable with more unknown values is 'education' (1731 observations)
4. If we look at the pattern table, we see that when 'housing' is unknown, 'loan' is unknown as well and vice versa


## 2.7 Preprocessing

Here starts the preprocessing of the data. If first place we'll remove the variables 'duration','default' and 'emp.var.rate' as in our data analysis we have seen that they won't be useful for our model.

```{r}
calls <- subset(calls, select=-c(duration, default, emp.var.rate))
```

The next step is the imputation of the missing values. To do this, I will use the MICE package (Multivariate Imputation by Chained Equations). You can see the documentation [here](https://cran.r-project.org/web/packages/mice/mice.pdf). First we'll do the initialization:

```{r}
init = mice(calls, maxit=0) 
meth = init$method
predM = init$predictorMatrix
```
The five variables are related with personal data. It is pointless to consider certain data to deduct personal data, as the interest rates. We can neither consider our target variable as a predictor for the missing values.

```{r}
# Remove some variables as predictors
predM[, c("contact","month","day_of_week", "campaign", "pdays",
          "previous","poutcome", "cons.price.idx","cons.conf.idx",
          "euribor3m","y")]=0
```
Now we define the methods for prediction. The variables 'job', 'marital' and 'education' have several categories. The variables 'housing' and 'loan' have only two categories.

```{r}
# set methods for prediction
meth[c("job", "marital","education")] = "polyreg"
meth[c("housing","loan")] = "logreg"
```
And now starts the imputation:

```{r echo=TRUE, results='hide'}
set.seed(103)
imputed = mice(calls, method=meth, predictorMatrix=predM, m=5)

imputed <- complete(imputed)
```

The next step is not part of the imputation. The results of some of the algorithms used in the next chapters have improved when we define the education variable as an ordered factor:

```{r}
## ordered factors
imputed$education <- factor(imputed$education, ordered=TRUE, levels = c("basic.4y","basic.6y","basic.9y","high.school","professional.course","university.degree"))
```

The summary of the dataset resulting of the imputation is the next:

```{r}
summary(imputed)
```

## 2.8 Division of train and test set

For the train set we will select a 90% of the observations and the other 10% will be the test set.

```{r}
#################################################################################
## divide in 90% train set and 10% test set
set.seed(204)
test_index <- createDataPartition(y = imputed$y, times = 1, p = 0.1, list = FALSE)

train_set <- imputed[-test_index,]
test_set <- imputed[test_index,]


rm(test_index)

```

We will define a subset of the training set to select the hyperparameters. We'll use it for the algorithms that need more computation power. When possible, we'll use the hole data set to select hyperparameters.

```{r}
## This is a sample that will be used to train the hiperparameters of several algorithms
n <- 6500
index <- sample(nrow(train_set), n)
```

## 2.9 Algorithms

In the next sections we'll train several algorithms in order to predict which contacts will result in a subscription. The intent of this section is not doing a detailed description of each algorithm, as it can be found in internet.

For each algorithm we may do the next steps:

1. Preprocessing of data: For some algorithms we'll do some operations on the data, as centering and scaling.
2. hyperparameter tuning
3. train the model
4. Evaluate with the test set.

To compare the performance of these models the Area Under the Curve (AUC) metric will be used. To calculate it, the pROC package is used.

### 2.9.1 Generalized Linear Model (glm)

This model has no hyperparameters, so we'll train directly the train set.

```{r echo=TRUE, results='hide'}
######################################
## glm
fit_glm <- glm(y ~ . , 
               data=train_set, 
               family="binomial")
p_glm <- predict(fit_glm, test_set, type="response")
```

And the result:

```{r}
ROC <- roc(test_set$y, p_glm)
plot(ROC, col = "red", print.auc=TRUE)

auc_results <- tibble(method = "glm", AUC = pROC::auc(ROC))
```

### 2.9.2 GLM with lasso or elasticnet regularization (glmnet)

On the next model we fit two hyperparamenters: alpha and lambda. As this algorithm is quick enough, we'll search the hyperparameters with the whole train set.

The hyperparameters for this algorithm are two: 
* alpha is the elasticnet mixing parameter. "1" for lasso regression, "0" for ridge regression
* lambda: the penalization used for regularization


```{r echo=TRUE, results='hide'}
######################################
## glmnet
glmnet_control <- trainControl(
  method = "cv", number=5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE
)

glmnet_grid <- expand.grid(alpha = 0:1, lambda = seq(0.0001, 1, length = 20))

train_glmnet <- train(
  y ~ ., train_set,
  tuneGrid = glmnet_grid,
  method = "glmnet",
  trControl = glmnet_control,
  metric="ROC"
)
```

The results of our hyperparameter search:

```{r}
# plot the model
plot(train_glmnet)
```

And now we do the predictions on the test set and calculate AUC:

```{r}
p_glmnet <- predict(train_glmnet, test_set, type="prob")


ROC <- roc(test_set$y, p_glmnet[,"yes"])
plot(ROC, col = "red", print.auc=TRUE)

auc_results <- rbind(auc_results,
                          tibble(method="glmnet",  
                                     AUC = pROC::auc(ROC) ))
```

### 2.9.3 k-Nearest Neighbours (KNN)

For this algorithm, the hyperparameter to search is the number of neighbors k. we'll do the hyperparameter search with a sample of the training set.

```{r echo=TRUE, results='hide'}
######################################
## knn

control_knn <- trainControl(
  method="cv", number = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE)


train_knn <- train(y ~ ., 
                   data=train_set[index,],
                   method = "knn", 
                   tuneGrid = data.frame(k = c(9,14,18,21,25)),
                   trControl = control_knn,
                   metric="ROC")
```

The results of our search:
```{r}
# plot the model
plot(train_knn)
```

Now train the algorithm with the best tune found on the whole train set.

```{r}
fit_knn <- knn3(y ~., train_set,  k = train_knn$bestTune$k)
```

Finally do the predictions on the test set and calculate AUC.
```{r}
p_knn  <- predict(fit_knn, test_set)

ROC <- roc(test_set$y, p_knn[,"yes"])
plot(ROC, col = "red", print.auc=TRUE)

auc_results <- rbind(auc_results,
                         tibble(method="knn",  
                                    AUC = pROC::auc(ROC) ))
```

### 2.9.4 Naive Bayes

Our next algorithm is Naive Bayes. This algorithm will produce a lot of warning messages, but we can ignore them. 

Our search grid includes three hyperparameters: usekernel (true or false), fl (Laplace correction) and adjust (a factor correction to multiply with the smoothing bandwidth; adjust = 1 means no adjust)
 
```{r echo=TRUE, results='hide'}
######################################
## Naive Bayes

control_nb <- trainControl(
  method="cv", number = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE)


nb_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL=0:3,
  adjust = c(0.5, 1, 1.5, 2)
)
```

With this algorithm we'll apply three transformations on the data: center (center numeric values around zero), scale (scale numeric values) and Independent component Analysis (ica):

```{r echo=TRUE, results='hide'}
preProc_nb <- preProcess(subset(train_set, select=-y), method=c("center", "scale","ica"),n.comp=5)
preProc_train_set <- predict(preProc_nb,train_set)
preProc_test_set <- predict(preProc_nb,test_set)
```

And now, search the best tune of the hyperparameters in a subset of the training set:

```{r echo=TRUE, results='hide', warning=FALSE}
train_nb <- train(y ~.,
                  data=preProc_train_set[index,],
                  method="nb",
                  trControl = control_nb,
                  tuneGrid = nb_grid,
                  metric="ROC")
```

The results of our search:
```{r}
#Plot the model
plot(train_nb)
```

Now we train the algorithm with the whole train set with the best tune:

```{r}
fit_nb <- NaiveBayes(y ~ .,
                     data=preProc_train_set,
                     fL = train_nb$bestTune$fL,
                     usekernel = train_nb$bestTune$usekernel,
                     adjust=train_nb$bestTune$adjust)

```

Finally do the predictions on the test set and calculate AUC.

```{r}
p_nb <- predict(fit_nb, preProc_test_set)$posterior

ROC <- roc(test_set$y, p_nb[,"yes"])
plot(ROC, col = "red", print.auc=TRUE)
auc_results <- rbind(auc_results,
                         tibble(method="Naive Bayes",  
                                    AUC = pROC::auc(ROC) ))
```

### 2.9.5 Random Forest (RBorist)

With the RBorist function we found a problem with the ordered factor, so just for this algorithm I'll undo this transformation: 

```{r}

# Preprocessing: Rborist returns an error of unsopported type where there are ordered factors
preProc_train_set <- train_set
preProc_train_set$education <- factor(preProc_train_set$education, ordered=FALSE)

preProc_test_set <- test_set
preProc_test_set$education <- factor(preProc_test_set$education, ordered=FALSE)
```

For this algorithm we'll search the best tune through two hyperparameters: minNode (minimal index width over which to attempt splitting) and predFixed (number of trial predictors for a split).

```{r echo=TRUE, results='hide'}
# hiperparameter tuning
control_rf <- trainControl(
  method="cv", number = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE,
  verboseIter = TRUE)

grid_rf <- expand.grid(minNode = c(1,2) , predFixed = c(10, 14, 17))

train_rf <-  train(y ~ ., 
                   data = preProc_train_set[index,], 
                   method = "Rborist", 
                   nTree = 50,
                   trControl = control_rf,
                   tuneGrid = grid_rf,
                   nSamp = 5000,
                   metric="ROC")
```

The next figure shows the results of our search:
```{r}
## plot the model
plot(train_rf)
```


Now we train the algorithm with the whole train set with the best tune:

```{r echo=TRUE, results='hide'}
fit_rf <- Rborist(subset(preProc_train_set, select=-y),
                  train_set$y,
                  nTree = 1000,
                  minNode = train_rf$bestTune$minNode,
                  predFixed = train_rf$bestTune$predFixed,
                  verbose=TRUE)
```

Finally do the predictions on the test set and calculate AUC:

```{r}
p_rf <- predict(fit_rf, subset(preProc_test_set, select=-y))$census  
p_rf<- p_rf / rowSums(p_rf)


ROC <- roc(test_set$y, p_rf[,"yes"])
plot(ROC, col = "red", print.auc=TRUE)
auc_results <- rbind(auc_results,
                         tibble(method="Rborist",  
                                    AUC = pROC::auc(ROC) ))
```

### 2.9.6 Support Vector Machines (SVM) Radial

The last algorithm that we will train on this project is the Support Vector Machines radial. The execution of this algorithm can take around 30 minutes. Here we'll tune two hyperparameters: gamma and C (cost of constraints violation)

First we'll apply two preprocessing transformations on the data. Center and scale

```{r}
preProc_svmRadial <- preProcess(subset(train_set, select=-y), method=c("center", "scale"))
preProc_train_set <- predict(preProc_nb,train_set)
preProc_test_set <- predict(preProc_nb,test_set)
```

And now we search the best hyperparameters with a subset of the train set.

```{r echo=TRUE, results='hide'}
tuned_parameters <- tune.svm(y~., 
                             data = preProc_train_set[index,], 
                             gamma = 10^(-5:-1), 
                             cost = c(0.01, 0.1, 0.5, 1, 3))


plot(tuned_parameters)
```

Next the model is trained with the best tune and the whole train set:

```{r}
fit_svmRadial <- svm(y ~ ., 
                     data=preProc_train_set, 
                     kernel="radial", 
                     cost=tuned_parameters$best.parameters$cost,
                     gamma=tuned_parameters$best.parameters$gamma,
                     probability=TRUE)
```

Finally get the predictions on the test set and calculate AUC: 

```{r}
pred <- predict(fit_svmRadial, preProc_test_set, probability=TRUE)

p_svmRadial <- attr(pred,"probabilities")

ROC <- roc(test_set$y, p_svmRadial[,"yes"])
plot(ROC, col = "red", print.auc=TRUE)
auc_results <- rbind(auc_results,
                         tibble(method="svmRadial",  
                                    AUC = pROC::auc(ROC) ))

```

### 2.9.7 Ensemble of previous models

Finally we'll calculate the mean of the probabilities obtained from the previous models. The intent of this last step is to combine the output of the previous models to obtain better predictions: 

```{r}
######################################
## Ensemble of previous models

p_ensemble <- (p_glm + p_glmnet + p_knn + p_nb + p_rf + p_svmRadial) / 6

ROC <- roc(test_set$y, p_ensemble[,"yes"])
plot(ROC, col = "red", print.auc=TRUE)
auc_results <- rbind(auc_results,
                         tibble(method="ensemble",  
                                    AUC = pROC::auc(ROC) ))

```

# 3. Results

As we can see the ensemble gives the best AUC:
```{r}
auc_results
```

The confusion matrix that we get with the ensemble of the five models:

```{r}
y_pred <- factor(ifelse(apply(p_ensemble, 1, which.max)-1==1, "yes", "no"))

cm_ensemble <- confusionMatrix(y_pred, test_set$y, positive="yes")

cm_ensemble
```

## 3.2 Tradeof between accuracy and sensitivity

The results have an accuracy of 90%, but our sentivity is quite low. 

But we can still do something. Remember that our objective is to optimize the resources of our marketing department. Imagine that the department has a budget to do 1000 calls. 

The variable 'p_ensemble' contains a structure with two columns "yes" and "no". The column "yes" contains the probabilities for each call to obtain the subscription. If we had budget for 1000 calls, we can simply get the probabilities obtained by our ensemble model and select the 1000 calls with the greatest probability to subscribe.

```{r}
selection <- test_set[order(p_ensemble$yes, decreasing=TRUE)[1:1000],]
sel_subscriptors <- sum(selection$y=="yes")
tot_subscriptors <- sum(test_set$y=="yes")
```

By selecting the 1000 out of `r toString(nrow(test_set))` best probabilities, we get `r toString(sel_subscriptors)` subscriptors. Not bad when the total number of possible subscriptors are `r toString(tot_subscriptors)`.


Other approach can be trying different thresholds with our vector of probabilities. We consider y="yes" when the probability of being "yes" is greater than the threshold. This way we sacrifice some accuracy to get more sensitivity. On the next figure we show the effect of selecting different thresholds (x axis) on several performance indicators.


```{r}
#################################################################################
## Accuracy - sensitivity trade off
#################################################################################

## performance indicators function
performance_indicators <- function(cut, probs, y) {
  y_hat = factor(ifelse(probs > cut, "yes", "no"), levels=c("yes","no"))
  w = which(y=="yes")
  sensitivity = mean( y_hat[w] == "yes" ) 
  specificity = mean( y_hat[-w] == "no" ) 
  accuracy = mean( y==y_hat ) 
  selection_rate = length(which(y_hat == "yes")) / length(y_hat)
  out = t(as.matrix(c(cut, sensitivity, specificity, accuracy, selection_rate)))
  colnames(out) = c("cut","sensitivity", "specificity", "accuracy", "pred_yes")
  return(out)
  
}

cuts <- seq(.01,.99,length=1000)

list_cut <- lapply(cuts, function(x) {performance_indicators(x,p_ensemble[,"yes"],test_set$y)})
df_cut <- data.frame(do.call(rbind, list_cut))

df_cut %>%
  ggplot() +
  geom_line(aes(x=cut,y=sensitivity, color="Sensitivity")) +
  geom_line(aes(x=cut,y=specificity, color="Specificity")) +
  geom_line(aes(x=cut,y=accuracy, color="Accuracy")) +
  geom_line(aes(x=cut,y=pred_yes, color="Subscriptions predicted")) +
  labs(color = 'Performance indicators')
```

Suppose that we call only to the rows predicted as subscriptions. Observe that the curve of subscriptions predicted has an elbow around a threshold of 0.11, which is the ratio of "yes" on the data set. This seems to be the optimal threshold.


# 4. Conclusions

With our Exploratory Data Analysis we have discarded some variables to be considered in our model, as they do not contribute much to predict subscriptions. In the other hand we have done a review of the variables with the most predictive power and we have gained some insights from them.

We have created a predictive model, that allow us to assign probabilities of success on a list of calls. This allows us to select the calls with the most chances of success if we have a limited budget.

We can improve this model by adding more algorithms to our ensemble. In the other hand we can improve the model as well by including new data or adding other variables. 

# 5. Citations

[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, In press, http://dx.doi.org/10.1016/j.dss.2014.03.001
